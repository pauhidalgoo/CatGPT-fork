{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST CATALAN DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script aims to create the best dataset for the Catalan language. The dataset is created by translating the well-known\n",
    "C4 in its variant realnewslike dataset to Catalan resulting in around 25 GB of text data about news articles. Its motivation\n",
    "comes from the lack of Catalan datasets in the NLP community and the need to have a good dataset to train models in this language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded by creating batches of 1 GB (parameter) in order to avoid memory issuess during the translation and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "class C4NewsBatchLoader:\n",
    "    def __init__(self, split='train'):\n",
    "        \"\"\"\n",
    "        Initialize the C4NewsBatchLoader with a specific split.\n",
    "        \n",
    "        Parameters:\n",
    "        split (str): The split of the dataset to load (train, validation, test).\n",
    "        \"\"\"\n",
    "        self.dataset = datasets.load_dataset('allenai/c4', 'realnewslike', split=split, streaming=True)\n",
    "        self.dataset_iter = iter(self.dataset)\n",
    "        self.bytes_written = 0\n",
    "        self.file_count = 1\n",
    "\n",
    "    def _save_to_file(self, text, file_count):\n",
    "        \"\"\"\n",
    "        Save text to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to save.\n",
    "        file_count (int): The current file count for naming the file.\n",
    "        \"\"\"\n",
    "        file_name = f\"../data/CA_realnewslike{file_count}.txt\"\n",
    "        print(f\"Saving to {file_name}\")\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "\n",
    "    def split_to_files(self, max_size_gb=1):\n",
    "        \"\"\"\n",
    "        Split the dataset into files of approximately max_size_gb GB each.\n",
    "        \n",
    "        Parameters:\n",
    "        max_size_gb (int): The maximum size of each file in GB.\n",
    "        \"\"\"\n",
    "        max_size_bytes = max_size_gb * 1024**3  # Convert GB to bytes\n",
    "        current_text = []\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                example = next(self.dataset_iter)\n",
    "                text = example['text']\n",
    "                current_text.append(text)\n",
    "                self.bytes_written += len(text.encode('utf-8'))\n",
    "\n",
    "                if self.bytes_written >= max_size_bytes:\n",
    "                    self._save_to_file(''.join(current_text), self.file_count)\n",
    "                    self.file_count += 1\n",
    "                    current_text = []\n",
    "                    self.bytes_written = 0\n",
    "\n",
    "        except StopIteration:\n",
    "            if current_text:\n",
    "                self._save_to_file(''.join(current_text), self.file_count)\n",
    "\n",
    "# Example usage:\n",
    "batch_loader = C4NewsBatchLoader(split='train')\n",
    "batch_loader.split_to_files(max_size_gb=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSLATE DATA USING LOCAL TRANSFORMERS MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end it was not possible to translate the whole dataset using the local transformers model due to computational limitations. The script is still available as it works and can be used to translate smaller datasets. If having a powerful machine, it is possible to translate the whole dataset by changing the `batch_size` parameter to a higher value. If you do so, please let me know how it went!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the NLTK data needed for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def translate_sentences(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {key: value.to('cuda') for key, value in inputs.items()}  # Move inputs to GPU\n",
    "    translated = model.generate(**inputs)\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "def translate_file(input_path, output_path, chunk_size=1024*1024, batch_size=32):  # 1MB chunk size, batch size 32\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-ca\"\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    model.to('cuda')  # Move model to GPU\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        while True:\n",
    "            text_chunk = infile.read(chunk_size)\n",
    "            if not text_chunk:\n",
    "                break\n",
    "\n",
    "            sentences = sent_tokenize(text_chunk)\n",
    "            translated_sentences = []\n",
    "            for i in range(0, len(sentences), batch_size):\n",
    "                batch = sentences[i:i + batch_size]\n",
    "                translated_batch = translate_sentences(batch, model, tokenizer)\n",
    "                translated_sentences.extend(translated_batch)\n",
    "\n",
    "            translated_text = \" \".join(translated_sentences)\n",
    "            outfile.write(translated_text + \"\\n\")\n",
    "            print(\"Translated and wrote a chunk.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
