{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatGPT_model import GPT, GPTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "from math import cos, pi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"../models/checkpoint.pth\"\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, step, dataloader, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': step,\n",
    "        'dataloader_state': {\n",
    "            'current_position': dataloader.file_pointer.tell(),\n",
    "            'tokens': dataloader.tokens\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}\")\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(model, optimizer, dataloader, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    dataloader.current_position = checkpoint['dataloader_state']['current_position']\n",
    "    dataloader.tokens = checkpoint['dataloader_state']['tokens']\n",
    "    dataloader.file_pointer.seek(dataloader.current_position)\n",
    "    step = checkpoint['step']\n",
    "    print(f\"Checkpoint loaded from step {step}\")\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288\n",
      "=> calculated gradient accumulation steps: 256\n",
      "num decayed parameter tensors: 50, with 110886912 parameters\n",
      "num non-decayed parameter tensors: 98, with 121344 parameters\n",
      "using fused AdamW: False\n",
      "Checkpoint loaded from step 1470\n"
     ]
    }
   ],
   "source": [
    "# Create Data Loadet class\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, file, B, T, buffer_size=100000000):\n",
    "        self.file = file\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.buffer_size = buffer_size\n",
    "        self.current_position = 0\n",
    "        self.tokenizer = ByteLevelBPETokenizer(\n",
    "            '../tokenizer/vocab.json',\n",
    "            '../tokenizer/merges.txt'\n",
    "        )\n",
    "        self.tokens = torch.tensor([], dtype=torch.long)\n",
    "        self.file_pointer = open(self.file, 'r')\n",
    "\n",
    "    def _load_tokens(self):\n",
    "        text = self.file_pointer.read(self.buffer_size)\n",
    "        if not text:\n",
    "            print(f\"TOTAL EPOCH OF TEXT FINISHED\")\n",
    "            self.file_pointer.seek(0)  # Reset to the beginning if end is reached\n",
    "            text = self.file_pointer.read(self.buffer_size)\n",
    "        encoded = self.tokenizer.encode(text).ids\n",
    "        return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        while len(self.tokens) <= B * T:\n",
    "            self.tokens = torch.cat((self.tokens, self._load_tokens()), dim=0)\n",
    "\n",
    "        buf = self.tokens[:B * T + 1]\n",
    "        self.tokens = self.tokens[B * T + 1:]  # Discard used tokens\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # inputs\n",
    "        y = buf[1:].view(B, T)   # targets\n",
    "        return x, y\n",
    "\n",
    "    def save_state(self, path):\n",
    "        state = {\n",
    "            'current_position': self.file_pointer.tell(),\n",
    "            'tokens': self.tokens\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        print(f\"DataLoaderLite state saved at {path}\")\n",
    "\n",
    "    def load_state(self, path):\n",
    "        state = torch.load(path)\n",
    "        self.current_position = state['current_position']\n",
    "        self.tokens = state['tokens']\n",
    "        self.file_pointer.seek(self.current_position)\n",
    "        print(f\"DataLoaderLite state loaded from {path}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.file_pointer.close()\n",
    "\n",
    "@dataclass\n",
    "class CatGPT_training_config:\n",
    "    B = 2\n",
    "    T = 1024\n",
    "    total_batch_size = 524288\n",
    "    float_matmul_precision = 'high'\n",
    "    vocab_size = 32768\n",
    "    max_lr = 6e-4\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = 35\n",
    "    steps = 10000\n",
    "    weight_decay = 0.1\n",
    "    betas = (0.9, 0.95)\n",
    "    eps = 1e-8\n",
    "    compile_model = True\n",
    "    use_gpu = False\n",
    "\n",
    "CatGPT_basic_config = CatGPT_training_config()\n",
    "\n",
    "assert (CatGPT_basic_config.total_batch_size % (CatGPT_basic_config.B * CatGPT_basic_config.T)) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
    "grad_accum_steps = CatGPT_basic_config.total_batch_size // (CatGPT_basic_config.B * CatGPT_basic_config.T)\n",
    "print(f\"total desired batch size: {CatGPT_basic_config.total_batch_size}\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if CatGPT_training_config.use_gpu:\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoaderLite(\"../data/tiny_corpus.txt\", B=CatGPT_training_config.B, T=CatGPT_training_config.T)\n",
    "\n",
    "# Set matmul precision to lower\n",
    "\n",
    "torch.set_float32_matmul_precision(CatGPT_training_config.float_matmul_precision)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = GPT(GPTConfig(vocab_size=CatGPT_training_config.vocab_size))\n",
    "model.to(device)\n",
    "\n",
    "if CatGPT_training_config.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay=CatGPT_training_config.weight_decay, learning_rate=CatGPT_training_config.max_lr, device=device)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_step = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_step = load_checkpoint(model, optimizer, train_loader, checkpoint_path, device)\n",
    "\n",
    "# Warmup + cosine decay learning rate schedule\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < CatGPT_basic_config.warmup_steps:\n",
    "        return CatGPT_basic_config.max_lr * (it + 1) / CatGPT_basic_config.warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > CatGPT_basic_config.steps:\n",
    "        return CatGPT_basic_config.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - CatGPT_basic_config.warmup_steps) / (CatGPT_basic_config.steps - CatGPT_basic_config.warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + cos(pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return CatGPT_basic_config.min_lr + coeff * (CatGPT_basic_config.max_lr - CatGPT_basic_config.min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(start_step, CatGPT_basic_config.steps):\n",
    "    initial_time = time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        print(micro_step)\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr = get_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    dt = time() - initial_time\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tokens_per_second = tokens_processed / dt\n",
    "    print(f\"Step {i} | Loss: {loss_accum.item()} | Time: {dt} | Tokens/s: {tokens_per_second} | LR: {lr}\")\n",
    "\n",
    "    # Save checkpoint periodically\n",
    "    if (i + 1) % 250 == 0:\n",
    "        save_checkpoint(model, optimizer, i + 1, train_loader, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text = 'La intel·ligència artificial tindrà la capactiat de', num_return_sequences = 1, max_length = 100):\n",
    "\n",
    "    enc = ByteLevelBPETokenizer(\n",
    "        '../tokenizer/vocab.json',\n",
    "        '../tokenizer/merges.txt'\n",
    "    )\n",
    "\n",
    "    # Encode the input text\n",
    "    tokens = enc.encode(input_text).ids\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long)  # (8,)\n",
    "\n",
    "    if len(tokens) > max_length:\n",
    "        max_length = len(tokens) + 25\n",
    "        print(f\"Max length set to {max_length} as input text is longer than the previous max length\")\n",
    "    tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (B, 8)\n",
    "    x = tokens.to(device)\n",
    "\n",
    "    # Set manual seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Generate sequences\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - x.size(1)):\n",
    "            logits, _ = model(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            x = torch.cat((x, next_token), dim=1)\n",
    "\n",
    "    # Decode and print the generated sequences\n",
    "    for i in range(num_return_sequences):\n",
    "        tokens = x[i].tolist()\n",
    "        decoded = enc.decode(tokens)\n",
    "        print(f\"Sample {i+1}: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length set to 33 as input text is longer than the previous max length\n",
      "Sample 1: Estic fart de la situació actual de la nostra gran societat. No paro d'Uniy Afo, encara que no trobes verd mana i decidim anar cap\n",
      "Sample 2: Estic fart de la situació actual de la nostra feina de seguretat i salut com més seguirem. Aquesta professora em dec el massatge amb la resta d'ingredients i\n",
      "Sample 3: Estic fart de la situació actual de l'habitatge se't deixarà seduir seguint les instruccions senzilles. Aquestes instruccions són precises pels centres d'informació i catalogació que permeten\n",
      "Sample 4: Estic fart de la situació actual de la topota d'aquesta escola. • Noorientar les bones sensacions amb els amics del centre, potenciar el vincle amb la\n",
      "Sample 5: Estic fart de la situació actual de contaminació provocada pel medi ambient. Avui hem arranjat el blog cap a les consumicions de combustibles fòssils que hem vist. Bona\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Estic fart de la situació actual de\", num_return_sequences=5, max_length=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
