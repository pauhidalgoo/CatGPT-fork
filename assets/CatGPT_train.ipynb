{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINING TRAINING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CatGPT_model import GPT, GPTConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "from math import cos, pi\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint path\n",
    "checkpoint_path = \"../models/checkpoint.pth\"\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, step, dataloader, checkpoint_path):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'step': step,\n",
    "        'dataloader_state': {\n",
    "            'current_position': dataloader.file_pointer.tell(),\n",
    "            'tokens': dataloader.tokens\n",
    "        }\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at step {step}\")\n",
    "\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(model, optimizer, dataloader, checkpoint_path, device):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    dataloader.current_position = checkpoint['dataloader_state']['current_position']\n",
    "    dataloader.tokens = checkpoint['dataloader_state']['tokens']\n",
    "    dataloader.file_pointer.seek(dataloader.current_position)\n",
    "    step = checkpoint['step']\n",
    "    print(f\"Checkpoint loaded from step {step}\")\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total desired batch size: 524288\n",
      "=> calculated gradient accumulation steps: 256\n",
      "num decayed parameter tensors: 50, with 110886912 parameters\n",
      "num non-decayed parameter tensors: 98, with 121344 parameters\n",
      "using fused AdamW: False\n",
      "Checkpoint loaded from step 13160\n"
     ]
    }
   ],
   "source": [
    "# Create Data Loadet class\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, file, B, T, buffer_size=100000000, device='cpu'):\n",
    "        self.file = file\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.current_position = 0\n",
    "        self.tokenizer = ByteLevelBPETokenizer(\n",
    "            '../tokenizer/vocab.json',\n",
    "            '../tokenizer/merges.txt'\n",
    "        )\n",
    "        self.tokens = torch.tensor([], dtype=torch.long, device=self.device)\n",
    "        self.file_pointer = open(self.file, 'r')\n",
    "\n",
    "    def _load_tokens(self):\n",
    "        text = self.file_pointer.read(self.buffer_size)\n",
    "        if not text:\n",
    "            print(f\"TOTAL EPOCH OF TEXT FINISHED\")\n",
    "            self.file_pointer.seek(0)  # Reset to the beginning if end is reached\n",
    "            text = self.file_pointer.read(self.buffer_size)\n",
    "        encoded = self.tokenizer.encode(text).ids\n",
    "        return torch.tensor(encoded, dtype=torch.long, device=self.device)\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        while len(self.tokens) <= B * T:\n",
    "            self.tokens = torch.cat((self.tokens, self._load_tokens()), dim=0)\n",
    "\n",
    "        buf = self.tokens[:B * T + 1]\n",
    "        self.tokens = self.tokens[B * T + 1:]  # Discard used tokens\n",
    "\n",
    "        x = buf[:-1].view(B, T)  # inputs\n",
    "        y = buf[1:].view(B, T)   # targets\n",
    "        return x, y\n",
    "\n",
    "    def save_state(self, path):\n",
    "        state = {\n",
    "            'current_position': self.file_pointer.tell(),\n",
    "            'tokens': self.tokens\n",
    "        }\n",
    "        torch.save(state, path)\n",
    "        print(f\"DataLoaderLite state saved at {path}\")\n",
    "\n",
    "    def load_state(self, path):\n",
    "        state = torch.load(path)\n",
    "        self.current_position = state['current_position']\n",
    "        self.tokens = state['tokens'].to(self.device)\n",
    "        self.file_pointer.seek(self.current_position)\n",
    "        print(f\"DataLoaderLite state loaded from {path}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.file_pointer.close()\n",
    "\n",
    "    def reset(self, new_file):\n",
    "        self.file_pointer.close()  # Close the current file\n",
    "        self.file = new_file\n",
    "        self.file_pointer = open(self.file, 'r')  # Open the new file\n",
    "        self.current_position = 0\n",
    "        self.tokens = torch.tensor([], dtype=torch.long, device=self.device)\n",
    "        print(f\"DataLoaderLite reset with new file {new_file}\")\n",
    "\n",
    "@dataclass\n",
    "class CatGPT_training_config:\n",
    "    B = 2\n",
    "    T = 1024\n",
    "    total_batch_size = 524288\n",
    "    float_matmul_precision = 'high'\n",
    "    vocab_size = 32768\n",
    "    max_lr = 6e-4\n",
    "    min_lr = max_lr * 0.1\n",
    "    warmup_steps = 35\n",
    "    steps = 10000\n",
    "    weight_decay = 0.1\n",
    "    betas = (0.9, 0.95)\n",
    "    eps = 1e-8\n",
    "    compile_model = True\n",
    "    use_gpu = False\n",
    "\n",
    "CatGPT_basic_config = CatGPT_training_config()\n",
    "\n",
    "assert (CatGPT_basic_config.total_batch_size % (CatGPT_basic_config.B * CatGPT_basic_config.T)) == 0, \"make sure total_batch_size is divisible by B * T\"\n",
    "grad_accum_steps = CatGPT_basic_config.total_batch_size // (CatGPT_basic_config.B * CatGPT_basic_config.T)\n",
    "print(f\"total desired batch size: {CatGPT_basic_config.total_batch_size}\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if CatGPT_training_config.use_gpu:\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoaderLite(\"../data/tiny_corpus.txt\", B=CatGPT_training_config.B, T=CatGPT_training_config.T)\n",
    "\n",
    "# Set matmul precision to lower\n",
    "\n",
    "torch.set_float32_matmul_precision(CatGPT_training_config.float_matmul_precision)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = GPT(GPTConfig(vocab_size=CatGPT_training_config.vocab_size))\n",
    "model.to(device)\n",
    "\n",
    "if CatGPT_training_config.compile_model:\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = model.configure_optimizers(weight_decay=CatGPT_training_config.weight_decay, learning_rate=CatGPT_training_config.max_lr, device=device)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "start_step = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    start_step = load_checkpoint(model, optimizer, train_loader, checkpoint_path, device)\n",
    "\n",
    "# Warmup + cosine decay learning rate schedule\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < CatGPT_basic_config.warmup_steps:\n",
    "        return CatGPT_basic_config.max_lr * (it + 1) / CatGPT_basic_config.warmup_steps\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > CatGPT_basic_config.steps:\n",
    "        return CatGPT_basic_config.min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - CatGPT_basic_config.warmup_steps) / (CatGPT_basic_config.steps - CatGPT_basic_config.warmup_steps)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + cos(pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
    "    return CatGPT_basic_config.min_lr + coeff * (CatGPT_basic_config.max_lr - CatGPT_basic_config.min_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(start_step, CatGPT_basic_config.steps):\n",
    "    initial_time = time()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        print(micro_step)\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        if device == \"cuda\":\n",
    "            with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                logits, loss = model(x, y)\n",
    "        else:\n",
    "            logits, loss = model(x, y)\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr = get_lr(i)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    optimizer.step()\n",
    "    dt = time() - initial_time\n",
    "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
    "    tokens_per_second = tokens_processed / dt\n",
    "    print(f\"Step {i} | Loss: {loss_accum.item()} | Time: {dt} | Tokens/s: {tokens_per_second} | LR: {lr}\")\n",
    "\n",
    "    # Save checkpoint periodically\n",
    "    if (i + 1) % 250 == 0:\n",
    "        save_checkpoint(model, optimizer, i + 1, train_loader, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESET THE TRAINING LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader.reset('/content/drive/MyDrive/CatGPT/CatGPT/data/catalan_oscar.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at ../models/CatGPT.pth\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, model_path):\n",
    "    \"\"\"\n",
    "    Save only the model state dictionary to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): The model to be saved.\n",
    "    model_path (str): The path where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved at {model_path}\")\n",
    "\n",
    "save_model(model, \"../models/CatGPT.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
